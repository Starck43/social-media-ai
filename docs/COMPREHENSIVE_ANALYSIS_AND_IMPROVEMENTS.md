# Comprehensive Analysis: Data Collection, Topic Chains & Optimizations

**Date**: Current Session  
**Author**: Factory Droid

---

## –í–æ–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

1. **–ü–µ—Ä–∏–æ–¥ –≤ —Ü–µ–ø–æ—á–∫–∞—Ö** - –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –¥–∞—Ç—ã —Ä–µ–∞–ª—å–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤ –≤–º–µ—Å—Ç–æ –¥–∞—Ç –∞–Ω–∞–ª–∏–∑–∞
2. **last_checked** - –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å –¥–∞—Ç—É –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞  
3. **LEGACY –ø–æ–ª—è** - —á—Ç–æ –¥–µ–ª–∞—Ç—å —Å `text_llm_provider_id`, `image_llm_provider_id`, `video_llm_provider_id`
4. **Topic Matching** - –∫–∞–∫ –ò–ò –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–æ–≤—É—é —Ç–µ–º—É vs –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å—Ç–∞—Ä–æ–π
5. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å–±–æ—Ä–∞** - –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ –∑–∞–ø—É—Å–∫–∏, –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
6. **API –ø–ª–∞—Ç—Ñ–æ—Ä–º** - —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –¥–∞—Ç–µ –≤ VK/Telegram
7. **–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª** - –∞–Ω–∞–ª–∏–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ò–ò –∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º

---

## 1. –ü–µ—Ä–∏–æ–¥ –≤ —Ü–µ–ø–æ—á–∫–∞—Ö: –î–∞—Ç—ã —Ä–µ–∞–ª—å–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤

### –¢–µ–∫—É—â–∞—è —Å–∏—Ç—É–∞—Ü–∏—è

**–°–µ–π—á–∞—Å**:
```
18 –æ–∫—Ç. 2025 –≥. - 18 –æ–∫—Ç. 2025 –≥. | 1 –∞–Ω–∞–ª–∏–∑
```

–ü–æ–∫–∞–∑—ã–≤–∞—é—Ç—Å—è –¥–∞—Ç—ã **–∞–Ω–∞–ª–∏–∑–∞** (`AIAnalytics.analysis_date`), –∞ –Ω–µ –¥–∞—Ç—ã **—Ä–µ–∞–ª—å–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤**.

### –ü—Ä–æ–±–ª–µ–º–∞

`AIAnalytics.analysis_date` = –¥–∞—Ç–∞ –∫–æ–≥–¥–∞ **–∑–∞–ø—É—â–µ–Ω –∞–Ω–∞–ª–∏–∑**  
–†–µ–∞–ª—å–Ω—ã–µ –ø–æ—Å—Ç—ã –º–æ–≥–ª–∏ –±—ã—Ç—å –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω—ã **–Ω–µ–¥–µ–ª—é –Ω–∞–∑–∞–¥** –∏–ª–∏ **–≤—á–µ—Ä–∞**

### –†–µ—à–µ–Ω–∏–µ

–î–æ–±–∞–≤–∏—Ç—å –≤ `summary_data` –ø–æ–ª—è:
- `content_date_range.earliest` - –¥–∞—Ç–∞ —Å–∞–º–æ–≥–æ —Å—Ç–∞—Ä–æ–≥–æ –ø–æ—Å—Ç–∞
- `content_date_range.latest` - –¥–∞—Ç–∞ —Å–∞–º–æ–≥–æ –Ω–æ–≤–æ–≥–æ –ø–æ—Å—Ç–∞

#### –ò–∑–º–µ–Ω–µ–Ω–∏—è –≤ analyzer.py

```python
def _extract_content_statistics(self, content: list[dict]) -> dict:
    """Extract statistics from collected content."""
    
    # Find date range from actual content
    post_dates = []
    for item in content:
        # Try different date fields
        pub_date = item.get('published_at') or item.get('date') or item.get('created_at')
        if pub_date:
            if isinstance(pub_date, int):  # Unix timestamp (VK)
                post_dates.append(datetime.fromtimestamp(pub_date, tz=UTC))
            elif isinstance(pub_date, str):
                post_dates.append(datetime.fromisoformat(pub_date))
    
    content_date_range = {}
    if post_dates:
        content_date_range = {
            'earliest': min(post_dates).isoformat(),
            'latest': max(post_dates).isoformat()
        }
    
    return {
        'total_posts': len(content),
        'content_date_range': content_date_range,  # NEW
        # ... existing fields
    }
```

#### –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ dashboard

```javascript
// –í buildChainCard()
const contentDates = chain.content_date_range || {}
const dateDisplay = contentDates.earliest && contentDates.latest
    ? `${DashboardUtils.formatDate(contentDates.earliest)} - ${DashboardUtils.formatDate(contentDates.latest)}`
    : `${DashboardUtils.formatDate(chain.first_date)} - ${DashboardUtils.formatDate(chain.last_date)}`

// Show
<div class="chain-meta">
    <span><i class="fas fa-calendar me-1"></i> ${dateDisplay}</span>
    <span><i class="fas fa-chart-bar me-1"></i> ${chain.analyses_count} –∞–Ω–∞–ª–∏–∑–æ–≤</span>
</div>
```

### last_checked –æ—Ç–¥–µ–ª—å–Ω–æ

**–ì–¥–µ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å**: –ü–æ–¥ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º —Ü–µ–ø–æ—á–∫–∏ –∫–∞–∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞

```javascript
<div class="chain-header">
    <div class="chain-title">
        <i class="fas fa-sparkles me-2 text-primary"></i>
        ${displayTitle}
    </div>
    <div class="chain-meta">
        <span title="–ü–µ—Ä–∏–æ–¥ –ø–æ—Å—Ç–æ–≤"><i class="fas fa-calendar me-1"></i> ${contentDateRange}</span>
        <span title="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–Ω–∞–ª–∏–∑–æ–≤"><i class="fas fa-chart-bar me-1"></i> ${chain.analyses_count} –∞–Ω–∞–ª–∏–∑–æ–≤</span>
        ${source.last_checked ? `
        <span title="–ü–æ—Å–ª–µ–¥–Ω—è—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞" class="text-muted">
            <i class="fas fa-sync me-1"></i> –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ: ${DashboardUtils.formatDateTime(source.last_checked)}
        </span>
        ` : ''}
    </div>
</div>
```

**–í–∏–∑—É–∞–ª—å–Ω–æ**:
```
‚ú® –û–±—Å—É–∂–¥–µ–Ω–∏–µ –¥–∏–∑–∞–π–Ω–∞ –∏ —Ä–µ–∞–∫—Ü–∏–∏ –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö

üìÖ 15 –æ–∫—Ç - 18 –æ–∫—Ç | üìä 3 –∞–Ω–∞–ª–∏–∑–∞ | üîÑ –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ: 18 –æ–∫—Ç, 14:30
```

---

## 2. LEGACY –ø–æ–ª—è: text_llm_provider_id –∏ –¥—Ä—É–≥–∏–µ

### –¢–µ–∫—É—â–∞—è —Å–∏—Ç—É–∞—Ü–∏—è

```python
# app/models/bot_scenario.py

# LEGACY: Individual FK fields (kept for backward compatibility)
text_llm_provider_id: Mapped[int | None] = Column(...)
image_llm_provider_id: Mapped[int | None] = Column(...)
video_llm_provider_id: Mapped[int | None] = Column(...)

# NEW: Strategy-based selection
llm_strategy: Mapped[LLMStrategyType] = LLMStrategyType.sa_column(...)
```

### –ü—Ä–æ–±–ª–µ–º–∞

**–î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ª–æ–≥–∏–∫–∏**:
- –°—Ç–∞—Ä—ã–π –ø–æ–¥—Ö–æ–¥: —è–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞—Ç—å provider –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞
- –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥: —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç provider

**–ú–∏–≥—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö**: –ï—Å—Ç—å –ª–∏ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–µ —Å—Ç–∞—Ä—ã–µ –ø–æ–ª—è?

### –ê–Ω–∞–ª–∏–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```python
# –ü—Ä–æ–≤–µ—Ä–∏–º –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ª–∏ LEGACY –ø–æ–ª—è
SELECT 
    COUNT(*) as total,
    COUNT(text_llm_provider_id) as with_text_provider,
    COUNT(image_llm_provider_id) as with_image_provider,
    COUNT(video_llm_provider_id) as with_video_provider
FROM social_manager.bot_scenarios;
```

### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è

#### –í–∞—Ä–∏–∞–Ω—Ç 1: **–ü–æ–ª–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ** (–µ—Å–ª–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è)

**–ö–æ–≥–¥–∞**: –ï—Å–ª–∏ –≤—Å–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç `llm_strategy`

**–®–∞–≥–∏**:
1. –°–æ–∑–¥–∞—Ç—å –º–∏–≥—Ä–∞—Ü–∏—é Alembic –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫
2. –£–¥–∞–ª–∏—Ç—å –ø–æ–ª—è –∏–∑ –º–æ–¥–µ–ª–∏
3. –£–±—Ä–∞—Ç—å –∏–∑ —Å—Ö–µ–º (schemas/scenario.py)

```python
# Migration
def upgrade():
    op.drop_column('bot_scenarios', 'text_llm_provider_id', schema='social_manager')
    op.drop_column('bot_scenarios', 'image_llm_provider_id', schema='social_manager')
    op.drop_column('bot_scenarios', 'video_llm_provider_id', schema='social_manager')

def downgrade():
    # Restore if needed
    op.add_column('bot_scenarios', 
        sa.Column('text_llm_provider_id', sa.Integer(), nullable=True),
        schema='social_manager'
    )
    # ...
```

#### –í–∞—Ä–∏–∞–Ω—Ç 2: **–ú–∏–≥—Ä–∞—Ü–∏—è —Å fallback** (–±–µ–∑–æ–ø–∞—Å–Ω–µ–µ)

**–ö–æ–≥–¥–∞**: –ï—Å–ª–∏ –µ—Å—Ç—å —Å—Ç–∞—Ä—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏

**–®–∞–≥–∏**:
1. –î–æ–±–∞–≤–∏—Ç—å migration script –¥–ª—è –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
2. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å llm_strategy –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ä—ã—Ö –ø–æ–ª–µ–π
3. –ü–æ–º–µ—Ç–∏—Ç—å LEGACY –ø–æ–ª—è –∫–∞–∫ deprecated
4. –ß–µ—Ä–µ–∑ N —Ä–µ–ª–∏–∑–æ–≤ —É–¥–∞–ª–∏—Ç—å

```python
# Migration: Copy old provider assignments to strategy
def upgrade():
    # If text_llm_provider_id is set but llm_strategy is NULL
    # ‚Üí set llm_strategy = "quality" (assume explicit choice = quality preference)
    op.execute("""
        UPDATE social_manager.bot_scenarios
        SET llm_strategy = 'quality'
        WHERE text_llm_provider_id IS NOT NULL 
          AND llm_strategy IS NULL
    """)
    
    # Mark columns as deprecated (add comment)
    op.execute("""
        COMMENT ON COLUMN social_manager.bot_scenarios.text_llm_provider_id 
        IS 'DEPRECATED: Use llm_strategy instead'
    """)
```

#### –í–∞—Ä–∏–∞–Ω—Ç 3: **Hybrid –ø–æ–¥—Ö–æ–¥** (—Ç–µ–∫—É—â–∏–π)

**–û—Å—Ç–∞–≤–∏—Ç—å –∫–∞–∫ –µ—Å—Ç—å**, –Ω–æ:
- –°–∫—Ä—ã—Ç—å –∏–∑ UI (—É–±—Ä–∞—Ç—å –∏–∑ forms)
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –¥–ª—è —á—Ç–µ–Ω–∏—è (backward compatibility)
- –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫ deprecated

```python
# –í BotScenarioAdmin
class BotScenarioAdmin(ModelView):
    # Exclude LEGACY fields from forms
    form_excluded_columns = [
        'text_llm_provider_id', 
        'image_llm_provider_id', 
        'video_llm_provider_id'
    ]
    
    # Show in list but mark as deprecated
    column_labels = {
        'text_llm_provider_id': '‚ö†Ô∏è Text Provider (Legacy)',
        'llm_strategy': 'LLM Strategy (Recommended)'
    }
```

### **–§–∏–Ω–∞–ª—å–Ω–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**: –í–∞—Ä–∏–∞–Ω—Ç 2

**–ü–ª–∞–Ω**:
1. –°–æ–∑–¥–∞—Ç—å –º–∏–≥—Ä–∞—Ü–∏—é –¥–ª—è –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö (–µ—Å–ª–∏ –µ—Å—Ç—å)
2. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å default llm_strategy –¥–ª—è —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π
3. –°–∫—Ä—ã—Ç—å LEGACY –ø–æ–ª—è –∏–∑ UI
4. –î–æ–±–∞–≤–∏—Ç—å warning –≤ –ª–æ–≥–∏ –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è
5. –ß–µ—Ä–µ–∑ 2-3 —Ä–µ–ª–∏–∑–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é —É–¥–∞–ª–∏—Ç—å

---

## 3. Topic Matching: –ö–∞–∫ –ò–ò –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–æ–≤—É—é —Ç–µ–º—É vs –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ

### –¢–µ–∫—É—â–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è

**–§–∞–π–ª**: `app/services/ai/analyzer.py`

```python
async def _find_matching_topic_chain(
    self,
    source: Source,
    current_topics: List[str],
    lookback_days: int = 7
) -> Optional[str]:
    """
    Find existing topic chain matching current analysis topics.
    
    Algorithm:
    1. Get recent analyses (last 7 days)
    2. Extract topics from each analysis
    3. Compare with current topics (string matching)
    4. If ‚â•50% overlap ‚Üí return existing chain_id
    5. Otherwise ‚Üí None (create new chain)
    """
    
    # Get recent analyses
    recent_analyses = await AIAnalytics.objects.filter(
        source_id=source.id,
        analysis_date__gte=cutoff_date
    ).order_by(AIAnalytics.analysis_date.desc()).limit(10)
    
    # Normalize topics
    current_topics_normalized = [t.lower().strip() for t in current_topics]
    
    # Check each analysis
    for analysis in recent_analyses:
        prev_topics = extract_topics(analysis.summary_data)
        prev_topics_normalized = [t.lower().strip() for t in prev_topics]
        
        # Calculate overlap
        matches = sum(1 for t in current_topics_normalized if t in prev_topics_normalized)
        match_ratio = matches / len(current_topics_normalized)
        
        if match_ratio >= 0.5:  # 50% threshold
            return analysis.topic_chain_id
    
    return None  # Create new chain
```

### –ü—Ä–∏–º–µ—Ä —Ä–∞–±–æ—Ç—ã

#### –°—Ü–µ–Ω–∞—Ä–∏–π 1: –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Ç–µ–º—ã

**–ê–Ω–∞–ª–∏–∑ #1** (15 –æ–∫—Ç—è–±—Ä—è):
- –¢–µ–º—ã: ["—Ä–µ–º–æ–Ω—Ç –¥–æ—Ä–æ–≥", "–±–ª–∞–≥–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ", "–ø—Ä–æ–±–∫–∏"]
- Chain ID: `chain_16_abc123`

**–ê–Ω–∞–ª–∏–∑ #2** (18 –æ–∫—Ç—è–±—Ä—è):
- –¢–µ–º—ã: ["—Ä–µ–º–æ–Ω—Ç –¥–æ—Ä–æ–≥", "–ø—Ä–æ–±–∫–∏", "–æ–±—ä–µ–∑–¥—ã"]
- –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ: 2 –∏–∑ 3 (66%) ‚úÖ
- **–†–µ–∑—É–ª—å—Ç–∞—Ç**: –î–æ–±–∞–≤–ª—è–µ—Ç—Å—è –≤ `chain_16_abc123`

#### –°—Ü–µ–Ω–∞—Ä–∏–π 2: –ù–æ–≤–∞—è —Ç–µ–º–∞

**–ê–Ω–∞–ª–∏–∑ #1** (15 –æ–∫—Ç—è–±—Ä—è):
- –¢–µ–º—ã: ["—Ä–µ–º–æ–Ω—Ç –¥–æ—Ä–æ–≥", "–±–ª–∞–≥–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ", "–ø—Ä–æ–±–∫–∏"]
- Chain ID: `chain_16_abc123`

**–ê–Ω–∞–ª–∏–∑ #3** (20 –æ–∫—Ç—è–±—Ä—è):
- –¢–µ–º—ã: ["–≤–∞–∫—Ü–∏–Ω–∞—Ü–∏—è", "covid", "–ø—Ä–∏–≤–∏–≤–∫–∏"]
- –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ: 0 –∏–∑ 3 (0%) ‚ùå
- **–†–µ–∑—É–ª—å—Ç–∞—Ç**: –°–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤–∞—è —Ü–µ–ø–æ—á–∫–∞ `chain_16_def456`

### –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ç–µ–∫—É—â–µ–≥–æ –ø–æ–¥—Ö–æ–¥–∞

1. **String matching** - –ø—Ä–æ—Å—Ç–æ–π, –Ω–æ –Ω–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏–∫—É
   - "–∞–≤—Ç–æ–º–æ–±–∏–ª—å–Ω—ã–µ –ø—Ä–æ–±–∫–∏" ‚â† "–ø—Ä–æ–±–∫–∏" (—Ö–æ—Ç—è –æ–¥–Ω–æ –∏ —Ç–æ –∂–µ)
   - "—Ä–µ–º–æ–Ω—Ç" ‚â† "–ø–æ—á–∏–Ω–∫–∞ –¥–æ—Ä–æ–≥" (—Ö–æ—Ç—è —Å–≤—è–∑–∞–Ω–æ)

2. **Fixed threshold (50%)** - –Ω–µ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è
   - –î–ª—è —É–∑–∫–∏—Ö —Ç–µ–º –Ω—É–∂–µ–Ω –≤—ã—à–µ –ø–æ—Ä–æ–≥
   - –î–ª—è —à–∏—Ä–æ–∫–∏—Ö —Ç–µ–º –º–æ–∂–Ω–æ –Ω–∏–∂–µ

3. **Lookback days = 7** - –∂–µ—Å—Ç–∫–æ –∑–∞–¥–∞–Ω–Ω–æ–µ –æ–∫–Ω–æ
   - –ê–∫—Ç–∏–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –º–æ–≥—É—Ç –∏–º–µ—Ç—å –∫–æ—Ä–æ—Ç–∫–∏–π —Ü–∏–∫–ª (1-2 –¥–Ω—è)
   - –ú–µ–¥–ª–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –º–æ–≥—É—Ç –æ–±–Ω–æ–≤–ª—è—Ç—å—Å—è —Ä–∞–∑ –≤ –º–µ—Å—è—Ü

### –£–ª—É—á—à–µ–Ω–∏—è

#### –í–∞—Ä–∏–∞–Ω—Ç A: Semantic Similarity (embeddings)

```python
from openai import AsyncOpenAI

async def _find_matching_topic_chain_semantic(
    self,
    source: Source,
    current_topics: List[str],
    similarity_threshold: float = 0.7
) -> Optional[str]:
    """Use embeddings for semantic topic matching."""
    
    # Get embeddings for current topics
    current_text = " ".join(current_topics)
    current_embedding = await self._get_embedding(current_text)
    
    # Get recent analyses
    recent_analyses = await AIAnalytics.objects.filter(...)
    
    for analysis in recent_analyses:
        prev_topics = extract_topics(analysis.summary_data)
        prev_text = " ".join(prev_topics)
        prev_embedding = await self._get_embedding(prev_text)
        
        # Cosine similarity
        similarity = cosine_similarity(current_embedding, prev_embedding)
        
        if similarity >= similarity_threshold:
            logger.info(f"Semantic match: {similarity:.2f} similarity")
            return analysis.topic_chain_id
    
    return None

async def _get_embedding(self, text: str) -> list[float]:
    """Get text embedding from OpenAI."""
    client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
    response = await client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return response.data[0].embedding
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞**:
- ‚úÖ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ ("–ø—Ä–æ–±–∫–∏" ‚âà "traffic jams")
- ‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç —Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
- ‚úÖ –£–ª–∞–≤–ª–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç

**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏**:
- ‚ùå –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ API calls ‚Üí —Å—Ç–æ–∏–º–æ—Å—Ç—å
- ‚ùå –ú–µ–¥–ª–µ–Ω–Ω–µ–µ —á–µ–º string matching

#### –í–∞—Ä–∏–∞–Ω—Ç B: Adaptive Threshold

```python
def _calculate_match_threshold(self, source: Source, topic_count: int) -> float:
    """Calculate dynamic threshold based on source activity and topic count."""
    
    # More topics = lower threshold (broader matching)
    if topic_count >= 5:
        base_threshold = 0.4
    elif topic_count >= 3:
        base_threshold = 0.5
    else:
        base_threshold = 0.6  # Strict for narrow topics
    
    # Adjust for source activity
    # Active sources (many posts) = stricter threshold
    if source.params.get('posts_per_day', 0) > 100:
        return min(base_threshold + 0.1, 0.8)
    
    return base_threshold
```

#### –í–∞—Ä–∏–∞–Ω—Ç C: Weighted Topics

–ù–µ –≤—Å–µ —Ç–µ–º—ã —Ä–∞–≤–Ω—ã. –ì–ª–∞–≤–Ω—ã–µ —Ç–µ–º—ã –¥–æ–ª–∂–Ω—ã –≤–µ—Å–∏—Ç—å –±–æ–ª—å—à–µ.

```python
# –í JSON schema –¥–æ–±–∞–≤–∏—Ç—å –≤–µ—Å–∞
SCHEMA_FIELDS = {
    'main_topics': {
        'topics': [
            {'name': '—Ä–µ–º–æ–Ω—Ç –¥–æ—Ä–æ–≥', 'weight': 0.8},  # –ì–ª–∞–≤–Ω–∞—è —Ç–µ–º–∞
            {'name': '–ø—Ä–æ–±–∫–∏', 'weight': 0.5},        # –í—Ç–æ—Ä–æ—Å—Ç–µ–ø–µ–Ω–Ω–∞—è
            {'name': '–æ–±—ä–µ–∑–¥—ã', 'weight': 0.3}        # –£–ø–æ–º–∏–Ω–∞–Ω–∏–µ
        ]
    }
}

# –ü—Ä–∏ matching —É—á–∏—Ç—ã–≤–∞—Ç—å –≤–µ—Å–∞
weighted_matches = sum(
    current_weights[topic] * prev_weights.get(topic, 0)
    for topic in current_topics
)
```

### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è

**Hybrid –ø–æ–¥—Ö–æ–¥**:
1. **String matching** –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–µ—Ä–≤–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞ (—Ç–µ–∫—É—â–∏–π)
2. **Semantic check** –¥–ª—è –≥—Ä–∞–Ω–∏—á–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤ (40-60% overlap)
3. **Adaptive threshold** –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞

```python
async def _find_matching_topic_chain_hybrid(self, source, current_topics):
    # Quick string matching
    string_match_chain = await self._find_matching_topic_chain(source, current_topics)
    
    if string_match_chain:
        return string_match_chain  # Clear match
    
    # Borderline cases: try semantic
    semantic_chain = await self._find_matching_topic_chain_semantic(
        source, current_topics, threshold=0.7
    )
    
    return semantic_chain
```

---

## 4. –ü–æ–≤—Ç–æ—Ä–Ω—ã–µ –∑–∞–ø—É—Å–∫–∏ –∏ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞

### –¢–µ–∫—É—â–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è: Checkpoint System

**–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è**: `docs/CHECKPOINT_SYSTEM.md`

**–ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã**:

1. **Source.last_checked** - timestamp –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–±–æ—Ä–∞
2. **BotScenario.collection_interval_hours** - –∫–∞–∫ —á–∞—Å—Ç–æ —Å–æ–±–∏—Ä–∞—Ç—å
3. **CheckpointManager** - –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω—É–∂–µ–Ω –ª–∏ —Å–±–æ—Ä

```python
from app.services.checkpoint_manager import CheckpointManager

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω—É–∂–µ–Ω –ª–∏ —Å–±–æ—Ä
if CheckpointManager.should_collect(source):
    # Collect new content ONLY
    await collect_and_analyze(source)
else:
    # Skip - checked recently
    pass
```

### VK API: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –¥–∞—Ç–µ

**–ï—Å—Ç—å –ª–∏ –≤ VK API —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –¥–∞—Ç–µ?** ‚Üí **–î–ê**

#### wall.get (–ø–æ—Å—Ç—ã)

```python
params = {
    'owner_id': '-12345',
    'count': 100,
    'filter': 'owner',
    # –ö–†–ò–¢–ò–ß–ù–û: –§–∏–ª—å—Ç—Ä –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    'start_time': int(last_checked.timestamp()),  # Unix timestamp
    'end_time': int(datetime.now().timestamp())
}
```

**–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è VK**: https://dev.vk.com/ru/method/wall.get

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã**:
- `start_time` (int) - –ù–∞—á–∞–ª–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ (Unix timestamp)
- `end_time` (int) - –ö–æ–Ω–µ—Ü –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ (Unix timestamp)

#### –ü—Ä–∏–º–µ—Ä VK –∑–∞–ø—Ä–æ—Å–∞

```python
async def collect_vk_posts_incremental(source: Source) -> list[dict]:
    """Collect only NEW VK posts since last_checked."""
    
    # Get checkpoint
    last_checked = source.last_checked or datetime.now(UTC) - timedelta(days=7)
    
    # VK API params with time filter
    params = {
        'owner_id': f"-{source.external_id}",
        'count': 100,
        'filter': 'owner',
        'start_time': int(last_checked.timestamp()),  # Only posts AFTER this
        'access_token': settings.VK_SERVICE_ACCESS_TOKEN,
        'v': '5.199'
    }
    
    # Call VK API
    url = 'https://api.vk.com/method/wall.get'
    async with httpx.AsyncClient() as client:
        response = await client.get(url, params=params)
        data = response.json()
    
    posts = data.get('response', {}).get('items', [])
    
    logger.info(f"Collected {len(posts)} NEW posts since {last_checked}")
    
    # Update checkpoint
    await source_manager.update_last_checked(source.id)
    
    return posts
```

### Telegram API: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –¥–∞—Ç–µ

**–ï—Å—Ç—å –ª–∏ –≤ Telegram API —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è?** ‚Üí **–î–ê (—á–µ—Ä–µ–∑ offset_id)**

#### messages.getHistory

Telegram –∏—Å–ø–æ–ª—å–∑—É–µ—Ç **message ID offset**, –∞ –Ω–µ timestamp.

```python
from telethon import TelegramClient

async def collect_telegram_incremental(source: Source) -> list:
    """Collect only NEW Telegram messages."""
    
    # Get last message ID from checkpoint
    last_message_id = source.params.get('last_message_id', 0)
    
    # Telethon client
    client = TelegramClient('session', api_id, api_hash)
    await client.connect()
    
    # Get messages AFTER last_message_id
    messages = await client.get_messages(
        entity=source.external_id,  # Channel/Group ID
        limit=100,
        offset_id=last_message_id,  # Only messages > this ID
        reverse=True  # From old to new
    )
    
    # Save new checkpoint
    if messages:
        new_last_id = messages[-1].id
        await source_manager.update_by_id(
            source.id,
            params={**source.params, 'last_message_id': new_last_id}
        )
    
    logger.info(f"Collected {len(messages)} NEW Telegram messages")
    
    return messages
```

**–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è Telegram**: https://docs.telethon.dev/en/stable/modules/client.html#telethon.client.messages.MessageMethods.get_messages

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã**:
- `offset_id` (int) - Message ID offset (—Ç–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏—è > —ç—Ç–æ–≥–æ ID)
- `limit` (int) - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π (–º–∞–∫—Å 100)
- `reverse` (bool) - –ü–æ—Ä—è–¥–æ–∫ (True = –æ—Ç —Å—Ç–∞—Ä—ã—Ö –∫ –Ω–æ–≤—ã–º)

### –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –∏—Ç–æ–≥–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π

**–í–æ–ø—Ä–æ—Å**: –ö–∞–∫ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏?

#### –ü—Ä–æ–±–ª–µ–º–∞

**Scenario**: 
- –î–µ–Ω—å 1: —Å–æ–±—Ä–∞–ª–∏ 50 –ø–æ—Å—Ç–æ–≤, 200 –ª–∞–π–∫–æ–≤
- –î–µ–Ω—å 2: —Å–æ–±—Ä–∞–ª–∏ 20 **–Ω–æ–≤—ã—Ö** –ø–æ—Å—Ç–æ–≤, 80 –ª–∞–π–∫–æ–≤
- **–ò—Ç–æ–≥–æ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å**: 70 –ø–æ—Å—Ç–æ–≤, 280 –ª–∞–π–∫–æ–≤

**–ù–û** –µ—Å–ª–∏ –ø—Ä–æ—Å—Ç–æ —Å—É–º–º–∏—Ä–æ–≤–∞—Ç—å analyses:
- –ê–Ω–∞–ª–∏–∑ 1: 50 –ø–æ—Å—Ç–æ–≤, 200 –ª–∞–π–∫–æ–≤
- –ê–Ω–∞–ª–∏–∑ 2: 20 –ø–æ—Å—Ç–æ–≤, 80 –ª–∞–π–∫–æ–≤
- **–°—É–º–º–∞**: 70 –ø–æ—Å—Ç–æ–≤, 280 –ª–∞–π–∫–æ–≤ ‚úÖ

#### –†–µ—à–µ–Ω–∏–µ: Incremental —Å—á–µ—Ç—á–∏–∫–∏

```python
# –í content_statistics
{
    'total_posts': 20,  # NEW posts in this analysis
    'total_reactions': 80,  # NEW reactions
    'cumulative_posts': 70,  # Total across all analyses
    'cumulative_reactions': 280  # Total reactions
}
```

**–†–∞—Å—á–µ—Ç cumulative**:

```python
async def _calculate_cumulative_stats(self, source: Source, new_stats: dict) -> dict:
    """Calculate cumulative statistics."""
    
    # Get latest analysis
    latest = await AIAnalytics.objects.filter(
        source_id=source.id
    ).order_by(AIAnalytics.analysis_date.desc()).first()
    
    prev_cumulative = {}
    if latest and latest.summary_data:
        prev_stats = latest.summary_data.get('content_statistics', {})
        prev_cumulative = {
            'posts': prev_stats.get('cumulative_posts', 0),
            'reactions': prev_stats.get('cumulative_reactions', 0)
        }
    
    # Add new to cumulative
    return {
        'total_posts': new_stats['total_posts'],  # This analysis
        'total_reactions': new_stats['total_reactions'],
        'cumulative_posts': prev_cumulative['posts'] + new_stats['total_posts'],
        'cumulative_reactions': prev_cumulative['reactions'] + new_stats['total_reactions'],
        'avg_reactions_per_post': (
            prev_cumulative['reactions'] + new_stats['total_reactions']
        ) / (prev_cumulative['posts'] + new_stats['total_posts'])
    }
```

---

## 5. –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–±–æ—Ä–∞ –∏ –∞–Ω–∞–ª–∏–∑–∞

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    ContentScheduler                          ‚îÇ
‚îÇ  (Runs every N hours via CLI/cron)                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              CheckpointManager                                ‚îÇ
‚îÇ  ‚Ä¢ Get sources needing collection                             ‚îÇ
‚îÇ  ‚Ä¢ Check last_checked + collection_interval_hours             ‚îÇ
‚îÇ  ‚Ä¢ Filter: only sources needing update                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            ContentCollector                                   ‚îÇ
‚îÇ  ‚Ä¢ For each source:                                           ‚îÇ
‚îÇ    - Get SocialClient (VK/Telegram/etc)                       ‚îÇ
‚îÇ    - Call collect_data()                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         SocialClient (VKClient/TelegramClient)                ‚îÇ
‚îÇ  ‚Ä¢ Build API params with checkpoint filters:                  ‚îÇ
‚îÇ    - VK: start_time = last_checked.timestamp()               ‚îÇ
‚îÇ    - Telegram: offset_id = last_message_id                   ‚îÇ
‚îÇ  ‚Ä¢ Call platform API                                          ‚îÇ
‚îÇ  ‚Ä¢ Return normalized content                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  AIAnalyzer                                   ‚îÇ
‚îÇ  ‚Ä¢ Classify content by media type                             ‚îÇ
‚îÇ  ‚Ä¢ Select LLM providers (via LLMResolver)                     ‚îÇ
‚îÇ  ‚Ä¢ Run parallel analyses:                                     ‚îÇ
‚îÇ    - Text analysis                                            ‚îÇ
‚îÇ    - Image analysis (if images present)                       ‚îÇ
‚îÇ    - Video analysis (if videos present)                       ‚îÇ
‚îÇ  ‚Ä¢ Create unified summary                                     ‚îÇ
‚îÇ  ‚Ä¢ Auto-detect topic chains (string matching)                 ‚îÇ
‚îÇ  ‚Ä¢ Save AIAnalytics to DB                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ             CheckpointManager.save()                          ‚îÇ
‚îÇ  ‚Ä¢ Update Source.last_checked = now()                         ‚îÇ
‚îÇ  ‚Ä¢ Update Source.params (cursors, offsets)                    ‚îÇ
‚îÇ  ‚Ä¢ Log collection result                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### –ü—Ä–∏–º–µ—Ä: –ü–æ–ª–Ω—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π VK –≥—Ä—É–ø–ø—ã

```python
# 1. SCHEDULER –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –∫–∞–∂–¥—ã–π —á–∞—Å
python cli/scheduler.py run -i 60

# 2. CheckpointManager –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∏
sources_to_collect = await CheckpointManager.get_sources_needing_collection()
# ‚Üí Source ID 16 (last_checked: 2h ago, interval: 1h) ‚úÖ

# 3. ContentCollector –∏–Ω–∏—Ü–∏–∏—Ä—É–µ—Ç —Å–±–æ—Ä
collector = ContentCollector()
await collector.collect_from_source(source_16)

# 4. VKClient —Å–æ–±–∏—Ä–∞–µ—Ç –ù–û–í–´–ï –ø–æ—Å—Ç—ã
vk_client = VKClient(platform)
posts = await vk_client.collect_data(source_16, 'posts')
# API call:
# https://api.vk.com/method/wall.get?owner_id=-16&start_time=1729000000

# VK response: 15 new posts (since last_checked)

# 5. AIAnalyzer –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç
analyzer = AIAnalyzer()
analysis = await analyzer.analyze_content(posts, source_16)

# 5a. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
# ‚Üí 12 text posts, 3 posts with images

# 5b. –í—ã–±–æ—Ä LLM providers
# text: GPT-4o-mini ($0.15/1M tokens) - from llm_strategy="cost_efficient"
# image: GPT-4o ($2.50/1M tokens) - for vision

# 5c. Parallel –∞–Ω–∞–ª–∏–∑
text_result = await llm.analyze_text(posts)
# Topics: ["–±–ª–∞–≥–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ", "—Å—É–±–±–æ—Ç–Ω–∏–∫", "–ø–∞—Ä–∫"]

# 5d. Topic matching
existing_chain = await _find_matching_topic_chain(source_16, topics)
# ‚Üí Found: chain_16_abc123 (match: 66%)

# 5e. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
await AIAnalytics.objects.create(
    source_id=16,
    summary_data={
        'analysis_title': '–°—É–±–±–æ—Ç–Ω–∏–∫ –≤ –ø–∞—Ä–∫–µ',
        'analysis_summary': '–ñ–∏—Ç–µ–ª–∏ –æ–±—Å—É–∂–¥–∞—é—Ç –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é —Å—É–±–±–æ—Ç–Ω–∏–∫–∞...',
        'multi_llm_analysis': {...},
        'content_statistics': {
            'total_posts': 15,
            'total_reactions': 89,
            'cumulative_posts': 150,  # Across all analyses
            'cumulative_reactions': 890
        }
    },
    topic_chain_id='chain_16_abc123',  # Linked to existing chain
    estimated_cost=12  # cents
)

# 6. Checkpoint –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è
await source_manager.update_last_checked(16, timestamp=now())
# Source.last_checked = 2024-10-18 14:30:00
```

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ –Ω–∞–≥—Ä—É–∑–∫–µ

#### 1. **Rate Limiting**

```python
# VK: max 3 requests/second
from app.core.rate_limiter import RateLimiter

vk_limiter = RateLimiter(max_calls=3, period=1.0)

async def call_vk_api(params):
    async with vk_limiter:
        response = await httpx.get(url, params=params)
        return response.json()
```

#### 2. **Batch Processing**

```python
# Collect from multiple sources in parallel (with limits)
import asyncio

async def collect_batch(sources: list[Source]):
    # Process 5 sources at a time
    for batch in chunks(sources, 5):
        tasks = [collector.collect_from_source(s) for s in batch]
        await asyncio.gather(*tasks)
        await asyncio.sleep(1)  # Cooldown between batches
```

#### 3. **Content Filtering**

```python
# Don't analyze EVERY post - filter by engagement
def filter_low_engagement(posts: list[dict], min_reactions: int = 5) -> list[dict]:
    """Only analyze posts with sufficient engagement."""
    return [
        p for p in posts 
        if p.get('likes', {}).get('count', 0) + p.get('comments', {}).get('count', 0) >= min_reactions
    ]

# In collector
posts = await vk_client.collect_data(source, 'posts')
filtered = filter_low_engagement(posts, min_reactions=10)
# Analyze only engaged posts ‚Üí save LLM costs
await analyzer.analyze_content(filtered, source)
```

#### 4. **Smart Collection Intervals**

```python
# Dynamic interval based on source activity
def calculate_optimal_interval(source: Source) -> int:
    """Calculate collection interval based on source activity."""
    
    # Get recent activity
    recent_analytics = await AIAnalytics.objects.filter(
        source_id=source.id
    ).order_by(AIAnalytics.analysis_date.desc()).limit(5)
    
    avg_posts = sum(
        a.summary_data.get('content_statistics', {}).get('total_posts', 0)
        for a in recent_analytics
    ) / len(recent_analytics)
    
    # Active source (many posts) ‚Üí check often
    if avg_posts > 50:
        return 1  # Every hour
    elif avg_posts > 10:
        return 6  # Every 6 hours
    else:
        return 24  # Once a day
    
# Update scenario
await BotScenario.objects.update_by_id(
    scenario_id,
    collection_interval_hours=calculate_optimal_interval(source)
)
```

---

## Summary: –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

### Immediate Actions (High Priority)

1. **‚úÖ –î–∞—Ç—ã –ø–æ—Å—Ç–æ–≤ –≤ dashboard**
   - –î–æ–±–∞–≤–∏—Ç—å `content_date_range` –≤ content_statistics
   - –û—Ç–æ–±—Ä–∞–∂–∞—Ç—å –≤ —Ü–µ–ø–æ—á–∫–∞—Ö –≤–º–µ—Å—Ç–æ –¥–∞—Ç –∞–Ω–∞–ª–∏–∑–∞
   - –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å `last_checked` –æ—Ç–¥–µ–ª—å–Ω–æ

2. **‚úÖ LEGACY –ø–æ–ª—è**
   - –í–∞—Ä–∏–∞–Ω—Ç 2: –ú–∏–≥—Ä–∞—Ü–∏—è —Å fallback
   - –°–∫—Ä—ã—Ç—å –∏–∑ UI
   - –£–¥–∞–ª–∏—Ç—å —á–µ—Ä–µ–∑ 2-3 —Ä–µ–ª–∏–∑–∞

3. **‚úÖ Topic Matching —É–ª—É—á—à–µ–Ω–∏—è**
   - –î–æ–±–∞–≤–∏—Ç—å adaptive threshold
   - –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å semantic similarity –¥–ª—è –≥—Ä–∞–Ω–∏—á–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤
   - Weighted topics –≤ —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏

### Medium Priority

4. **üìã –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å–±–æ—Ä–∞**
   - –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—Å–µ –∫–æ–ª–ª–µ–∫—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç checkpoint
   - –î–æ–±–∞–≤–∏—Ç—å batch processing –¥–ª—è –º–Ω–æ–≥–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
   - –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å content filtering –ø–æ engagement

5. **üìã Cumulative statistics**
   - –î–æ–±–∞–≤–∏—Ç—å cumulative —Å—á–µ—Ç—á–∏–∫–∏
   - –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å total vs incremental –≤ dashboard

### Future Enhancements

6. **üîÆ Semantic embeddings**
   - OpenAI embeddings –¥–ª—è topic matching
   - Cached embeddings –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏

7. **üîÆ Smart intervals**
   - Dynamic collection_interval_hours
   - Auto-adjust –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏

---

**–°—Ç–∞—Ç—É—Å**: –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω  
**–°–ª–µ–¥—É—é—â–∏–π —à–∞–≥**: –°–æ–≥–ª–∞—Å–æ–≤–∞—Ç—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º
