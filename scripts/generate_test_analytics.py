#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ü–µ–ø–æ—á–µ–∫ —Ç–µ–º.

–°–æ–∑–¥–∞–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ —Ç–∞–±–ª–∏—Ü–µ AIAnalytics —Å –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–º–∏ –ø–æ–ª—è–º–∏:
- topic_chain_id –¥–ª—è —Ü–µ–ø–æ—á–µ–∫ —Ç–µ–º
- response_payload —Å –¥–∞–Ω–Ω—ã–º–∏ –∞–Ω–∞–ª–∏–∑–∞
- summary_data —Å –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
"""

import asyncio
import json
import os
import random
import sys
from datetime import date, datetime, timedelta

from app.core.database import async_engine
from sqlalchemy import text

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))


async def create_test_analytics_data():
	"""–°–æ–∑–¥–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ü–µ–ø–æ—á–µ–∫ —Ç–µ–º."""

	async with async_engine.begin() as conn:
		# –ü–æ–ª—É—á–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è —Å–≤—è–∑–∏
		sources = await conn.run_sync(lambda sync_conn: sync_conn.execute(
			text("SELECT id, name FROM social_manager.sources WHERE is_active = true LIMIT 5")
		).fetchall())

		if not sources:
			print("‚ùå –ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. –°–æ–∑–¥–∞–π—Ç–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —Å–Ω–∞—á–∞–ª–∞.")
			return

		# –ü–æ–ª—É—á–∏—Ç—å —Å—Ü–µ–Ω–∞—Ä–∏–∏ –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
		scenarios = await conn.run_sync(lambda sync_conn: sync_conn.execute(
			text("SELECT id, name FROM social_manager.bot_scenarios LIMIT 3")
		).fetchall())

		print(f"üìä –°–æ–∑–¥–∞—é —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è {len(sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")

		# –¢–µ—Å—Ç–æ–≤—ã–µ —Ü–µ–ø–æ—á–∫–∏ —Ç–µ–º
		topic_chains = [
			{
				"id": "brand_monitoring_001",
				"name": "–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –±—Ä–µ–Ω–¥–∞ X",
				"description": "–û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –±—Ä–µ–Ω–¥–∞ X –≤ —Å–æ—Ü—Å–µ—Ç—è—Ö"
			},
			{
				"id": "competitor_analysis_002",
				"name": "–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤",
				"description": "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º–∏ –≤ –Ω–∏—à–µ"
			},
			{
				"id": "crisis_detection_003",
				"name": "–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∫—Ä–∏–∑–∏—Å–æ–≤",
				"description": "–†–∞–Ω–Ω–µ–µ –≤—ã—è–≤–ª–µ–Ω–∏–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —Ç—Ä–µ–Ω–¥–æ–≤"
			},
			{
				"id": "product_feedback_004",
				"name": "–û—Ç–∑—ã–≤—ã –æ –ø—Ä–æ–¥—É–∫—Ç–∞—Ö",
				"description": "–ê–Ω–∞–ª–∏–∑ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ –ø—Ä–æ–¥—É–∫—Ç–∞—Ö"
			}
		]

		# –®–∞–±–ª–æ–Ω—ã —Ç–µ–º –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ü–µ–ø–æ—á–µ–∫
		chain_topics = {
			"brand_monitoring_001": [
				"–ë—Ä–µ–Ω–¥ X", "–ö–∞—á–µ—Å—Ç–≤–æ –ø—Ä–æ–¥—É–∫—Ç–æ–≤", "–¶–µ–Ω–∞", "–û–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ", "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"
			],
			"competitor_analysis_002": [
				"–ë—Ä–µ–Ω–¥ X vs –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç Y", "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ü–µ–Ω", "–ö–∞—á–µ—Å—Ç–≤–æ —Å–µ—Ä–≤–∏—Å–∞", "–ò–Ω–Ω–æ–≤–∞—Ü–∏–∏"
			],
			"crisis_detection_003": [
				"–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –æ—Ç–∑—ã–≤—ã", "–ü—Ä–æ–±–ª–µ–º—ã —Å –¥–æ—Å—Ç–∞–≤–∫–æ–π", "–ö–∞—á–µ—Å—Ç–≤–æ –ø—Ä–æ–¥—É–∫—Ü–∏–∏", "–°–ª—É–∂–±–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∏"
			],
			"product_feedback_004": [
				"–ù–æ–≤—ã–π –ø—Ä–æ–¥—É–∫—Ç Z", "–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å", "–î–∏–∑–∞–π–Ω", "–¶–µ–Ω–∞", "–ö–∞—á–µ—Å—Ç–≤–æ"
			]
		}

		# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π
		base_date = date.today() - timedelta(days=30)

		for day_offset in range(31):  # 31 –¥–µ–Ω—å –¥–∞–Ω–Ω—ã—Ö
			current_date = base_date + timedelta(days=day_offset)

			for source_id, source_name in sources:
				# –í—ã–±—Ä–∞—Ç—å —Å–ª—É—á–∞–π–Ω—É—é —Ü–µ–ø–æ—á–∫—É —Ç–µ–º –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞
				chain = random.choice(topic_chains)
				chain_id = chain["id"]
				topics = chain_topics[chain_id]

				# –°–æ–∑–¥–∞—Ç—å response_payload —Å –¥–∞–Ω–Ω—ã–º–∏ –∞–Ω–∞–ª–∏–∑–∞
				response_payload = {
					"multi_llm_analysis": {
						"text_analysis": {
							"parsed": {
								"topic_analysis": {
									"main_topics": random.sample(topics, min(3, len(topics))),
									"topic_prevalence": {topic: round(random.uniform(0.1, 0.8), 2)
									                     for topic in topics[:3]},
									"emerging_topics": random.sample(topics, min(2, len(topics)))
								},
								"sentiment_analysis": {
									"overall_sentiment": random.choice(["positive", "negative", "neutral"]),
									"sentiment_score": round(random.uniform(0.2, 0.9), 2),
									"dominant_emotions": random.sample(
										["—Ä–∞–¥–æ—Å—Ç—å", "—É–¥–∏–≤–ª–µ–Ω–∏–µ", "–≥–Ω–µ–≤", "–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ", "—Å—Ç—Ä–∞—Ö"], 2)
								}
							},
							"request": {
								"model": random.choice(["gpt-4", "claude-3", "deepseek-chat"]),
								"prompt": f"–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {source_name} –∑–∞ {current_date}"
							},
							"response": {
								"choices": [{"message": {"content": "–¢–µ—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑..."}}],
								"usage": {"total_tokens": 1500}
							}
						}
					}
				}

				# –°–æ–∑–¥–∞—Ç—å summary_data
				summary_data = {
					"ai_analysis": {
						"topic_analysis": {
							"main_topics": random.sample(topics, min(3, len(topics))),
							"topic_prevalence": {topic: round(random.uniform(0.1, 0.8), 2)
							                     for topic in topics[:3]}
						},
						"sentiment_analysis": {
							"overall_sentiment": random.choice(["positive", "negative", "neutral"]),
							"sentiment_score": round(random.uniform(0.2, 0.9), 2)
						}
					},
					"content_statistics": {
						"total_posts": random.randint(10, 100),
						"avg_reactions_per_post": round(random.uniform(5, 50), 1),
						"total_comments": random.randint(20, 200)
					},
					"source_metadata": {
						"source_type": random.choice(["VK", "Telegram"]),
						"platform": "VK" if "VK" in source_name else "Telegram",
						"source_name": source_name
					}
				}

				# –í—Å—Ç–∞–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ —Ç–∞–±–ª–∏—Ü—É
				await conn.run_sync(lambda sync_conn: sync_conn.execute(
					text("""
                    INSERT INTO social_manager.ai_analytics
                    (source_id, analysis_date, period_type, topic_chain_id, summary_data,
                     llm_model, prompt_text, response_payload, created_at, updated_at)
                    VALUES (:source_id, :analysis_date, :period_type, :topic_chain_id, :summary_data,
                           :llm_model, :prompt_text, :response_payload, :created_at, :updated_at)
                    """),
					{
						"source_id": source_id,
						"analysis_date": current_date,
						"period_type": "DAILY",
						"topic_chain_id": chain_id,
						"summary_data": json.dumps(summary_data),
						"llm_model": random.choice(["gpt-4", "claude-3", "deepseek-chat"]),
						"prompt_text": f"–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∑–∞ {current_date}",
						"response_payload": json.dumps(response_payload),
						"created_at": datetime.now(),
						"updated_at": datetime.now()
					}
				))

				print(f"‚úÖ –°–æ–∑–¥–∞–Ω –∞–Ω–∞–ª–∏–∑ –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {source_name} ({current_date}) - —Ü–µ–ø–æ—á–∫–∞ {chain['name']}")

		print(f"\nüéâ –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ–∑–¥–∞–Ω—ã! –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(sources) * 31}")
		print("üìä –¶–µ–ø–æ—á–∫–∏ —Ç–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã –≤ –¥–∞—à–±–æ—Ä–¥–µ: /dashboard/topic-chains")


if __name__ == "__main__":
	asyncio.run(create_test_analytics_data())
