# –ü—Ä–æ–±–ª–µ–º—ã –ª–æ–≥–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏

**–î–∞—Ç–∞**: 2025-10-18  
**–ü—Ä–æ–±–ª–µ–º—ã**:
1. ‚ùå –î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è (3:00 –≤–º–µ—Å—Ç–æ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏)
2. ‚ùå –¢–æ–ª—å–∫–æ 1 –∑–∞–ø–∏—Å—å –≤ –∞–Ω–∞–ª–∏—Ç–∏–∫–µ –≤–º–µ—Å—Ç–æ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ (–ø–æ –¥–Ω—è–º)

---

## üîç –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–π –ª–æ–≥–∏–∫–∏

### **–ü—Ä–æ–±–ª–µ–º–∞ #1: –î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞ = date.today()**

**–§–∞–π–ª**: `app/services/ai/analyzer.py`, —Å—Ç—Ä–æ–∫–∞ ~697

```python
async def _save_analysis(...):
    # ...
    
    # Check if analysis already exists for today
    existing_analysis = await AIAnalytics.objects.filter(
        source_id=source.id,
        analysis_date=date.today(),  # ‚¨ÖÔ∏è –ü–†–û–ë–õ–ï–ú–ê!
        period_type=PeriodType.DAILY
    ).first()
    
    # ...
    
    # Create analytics record
    analytics = await AIAnalytics.objects.create(
        source_id=source.id,
        analysis_date=date.today(),  # ‚¨ÖÔ∏è –ü–†–û–ë–õ–ï–ú–ê!
        period_type=PeriodType.DAILY,
        # ...
    )
```

**–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç**:
- `date.today()` = —Ç–µ–∫—É—â–∞—è –¥–∞—Ç–∞ –ë–ï–ó –≤—Ä–µ–º–µ–Ω–∏ (—Ç–æ–ª—å–∫–æ –¥–∞—Ç–∞)
- –ü—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ –ë–î PostgreSQL –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤ `2025-10-18 00:00:00`
- –ü—Ä–∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–∏ —Å timezone: `2025-10-18 00:00:00 UTC` ‚Üí `2025-10-18 03:00:00 MSK` (UTC+3)

**–î–æ–ª–∂–Ω–æ –±—ã—Ç—å**:
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `datetime.now(UTC).date()` –¥–ª—è —è–≤–Ω–æ–≥–æ —É–∫–∞–∑–∞–Ω–∏—è timezone
- –ò–ª–∏ —Ö—Ä–∞–Ω–∏—Ç—å `analysis_date` –∫–∞–∫ DATE —Ç–∏–ø (–±–µ–∑ –≤—Ä–µ–º–µ–Ω–∏)

---

### **–ü—Ä–æ–±–ª–µ–º–∞ #2: –û–¥–Ω–∞ –∑–∞–ø–∏—Å—å –Ω–∞ –≤–µ—Å—å –ø–µ—Ä–∏–æ–¥**

**–¢–µ–∫—É—â–∞—è –ª–æ–≥–∏–∫–∞**:
```python
# 1. –°–æ–±–∏—Ä–∞–µ—Ç—Å—è –∫–æ–Ω—Ç–µ–Ω—Ç (100 –ø–æ—Å—Ç–æ–≤ –∑–∞ –≤–µ—Å—å –ø–µ—Ä–∏–æ–¥)
content = await collector.collect(source)

# 2. –í—Å–µ 100 –ø–æ—Å—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –û–î–ù–ò–ú –∑–∞–ø—Ä–æ—Å–æ–º –∫ LLM
analysis = await analyzer.analyze_content(content, source)

# 3. –°–æ–∑–¥–∞—ë—Ç—Å—è –û–î–ù–ê –∑–∞–ø–∏—Å—å AIAnalytics —Å analysis_date = TODAY
analytics = await AIAnalytics.objects.create(
    source_id=source.id,
    analysis_date=date.today(),  # –°–µ–≥–æ–¥–Ω—è
    summary_data={
        "content_statistics": {
            "total_posts": 100,
            "date_range": {
                "first": "2024-01-29",  # –°–∞–º—ã–π —Å—Ç–∞—Ä—ã–π –ø–æ—Å—Ç
                "last": "2025-10-17"     # –°–∞–º—ã–π –Ω–æ–≤—ã–π –ø–æ—Å—Ç
            }
        }
    }
)
```

**–ü—Ä–æ–±–ª–µ–º–∞**:
- –°–æ–±–∏—Ä–∞–µ—Ç—Å—è –∫–æ–Ω—Ç–µ–Ω—Ç –∑–∞ **–í–°–Æ –ò–°–¢–û–†–ò–Æ** (—Å 2024-01-29 –ø–æ 2025-10-17 = 627 –¥–Ω–µ–π!)
- –°–æ–∑–¥–∞—ë—Ç—Å—è —Ç–æ–ª—å–∫–æ **1 –∑–∞–ø–∏—Å—å** —Å –¥–∞—Ç–æ–π TODAY (2025-10-18)
- Timeline –Ω–∞ dashboard –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ 1 —Ç–æ—á–∫—É

**–î–æ–ª–∂–Ω–æ –±—ã—Ç—å**:
- –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç **–ø–æ –¥–Ω—è–º**
- –°–æ–∑–¥–∞–≤–∞—Ç—å **–æ—Ç–¥–µ–ª—å–Ω—É—é –∑–∞–ø–∏—Å—å AIAnalytics** –Ω–∞ –∫–∞–∂–¥—ã–π –¥–µ–Ω—å —Å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é
- –ò–ª–∏: –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ (—Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –ø–æ—Å—Ç—ã —Å last_checked)

---

## üìä –¢–µ–∫—É—â–∞—è —Å–∏—Ç—É–∞—Ü–∏—è Source #19

### **–ß—Ç–æ —Å–æ–±—Ä–∞–Ω–æ**:
```
Total posts: 100
Date range: 2024-01-29 ‚Üí 2025-10-17 (627 –¥–Ω–µ–π!)
```

### **–ß—Ç–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ**:
```sql
SELECT id, analysis_date, created_at
FROM social_manager.ai_analytics
WHERE source_id = 19;

-- –†–µ–∑—É–ª—å—Ç–∞—Ç:
id | analysis_date | created_at
---+---------------+---------------------------
93 | 2025-10-18    | 2025-10-18 18:20:51 UTC
```

**–ü—Ä–æ–±–ª–µ–º–∞**: –¢–æ–ª—å–∫–æ **1 –∑–∞–ø–∏—Å—å** –∑–∞ —Å–µ–≥–æ–¥–Ω—è, —Ö–æ—Ç—è –¥–∞–Ω–Ω—ã–µ –æ—Ö–≤–∞—Ç—ã–≤–∞—é—Ç 627 –¥–Ω–µ–π!

---

## ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞

### **–í–∞—Ä–∏–∞–Ω—Ç A: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–Ω—è–º**

**–ß—Ç–æ –¥–µ–ª–∞—Ç—å**:
1. –°–æ–±—Ä–∞—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç
2. **–°–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ø–æ –¥–Ω—è–º** (–ø–æ –ø–æ–ª—é `published_at` –∏–ª–∏ `date`)
3. –î–ª—è **–∫–∞–∂–¥–æ–≥–æ –¥–Ω—è** —Å–æ–∑–¥–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω—É—é –∑–∞–ø–∏—Å—å AIAnalytics

**–ö–æ–¥**:
```python
async def analyze_content_by_days(self, content, source, bot_scenario):
    """Group content by days and create separate analytics for each day."""
    
    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–Ω—è–º
    from collections import defaultdict
    from datetime import datetime, timezone
    
    content_by_day = defaultdict(list)
    
    for item in content:
        pub_date = item.get('published_at') or item.get('date') or item.get('created_at')
        
        # Convert Unix timestamp to datetime
        if isinstance(pub_date, int):
            pub_date = datetime.fromtimestamp(pub_date, tz=timezone.utc)
        elif isinstance(pub_date, str):
            pub_date = datetime.fromisoformat(pub_date.replace('Z', '+00:00'))
        
        # Group by date (without time)
        day = pub_date.date() if pub_date else date.today()
        content_by_day[day].append(item)
    
    logger.info(f"Grouped content into {len(content_by_day)} days")
    
    # –ê–Ω–∞–ª–∏–∑ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–Ω—è
    analytics_list = []
    
    for day, day_content in sorted(content_by_day.items()):
        logger.info(f"Analyzing {len(day_content)} items for {day}")
        
        # –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∑–∞ –¥–µ–Ω—å
        analysis = await self.analyze_content(day_content, source, bot_scenario)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å –ü–†–ê–í–ò–õ–¨–ù–û–ô –¥–∞—Ç–æ–π
        analytics = await self._save_analysis(
            analysis_results=analysis,
            source=source,
            analysis_date=day,  # ‚¨ÖÔ∏è –î–∞—Ç–∞ –ö–û–ù–¢–ï–ù–¢–ê, –Ω–µ TODAY!
            # ...
        )
        
        analytics_list.append(analytics)
    
    return analytics_list
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç**:
```sql
SELECT id, analysis_date, created_at
FROM social_manager.ai_analytics
WHERE source_id = 19
ORDER BY analysis_date DESC;

-- –†–µ–∑—É–ª—å—Ç–∞—Ç:
id | analysis_date | created_at
---+---------------+---------------------------
101| 2025-10-17    | 2025-10-18 18:20:51 UTC
100| 2025-10-16    | 2025-10-18 18:20:50 UTC
99 | 2025-10-15    | 2025-10-18 18:20:49 UTC
...
50 | 2024-02-01    | 2025-10-18 18:20:20 UTC
49 | 2024-01-29    | 2025-10-18 18:20:19 UTC
```

**Timeline –Ω–∞ dashboard**:
```
source_19_scenario_10
‚îú‚îÄ 2024-01-29: [–∞–Ω–∞–ª–∏–∑ –∑–∞ —ç—Ç–æ—Ç –¥–µ–Ω—å]
‚îú‚îÄ 2024-01-30: [–∞–Ω–∞–ª–∏–∑ –∑–∞ —ç—Ç–æ—Ç –¥–µ–Ω—å]
‚îú‚îÄ ...
‚îú‚îÄ 2025-10-16: [–∞–Ω–∞–ª–∏–∑ –∑–∞ —ç—Ç–æ—Ç –¥–µ–Ω—å]
‚îî‚îÄ 2025-10-17: [–∞–Ω–∞–ª–∏–∑ –∑–∞ —ç—Ç–æ—Ç –¥–µ–Ω—å]
```

---

### **–í–∞—Ä–∏–∞–Ω—Ç B: –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (—Ç–æ–ª—å–∫–æ –Ω–æ–≤–æ–µ)**

**–ß—Ç–æ –¥–µ–ª–∞—Ç—å**:
1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `last_checked` –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
2. –°–æ–±–∏—Ä–∞—Ç—å —Ç–æ–ª—å–∫–æ **–Ω–æ–≤—ã–µ –ø–æ—Å—Ç—ã**
3. –°–æ–∑–¥–∞–≤–∞—Ç—å –∑–∞–ø–∏—Å—å **—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –Ω–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç**

**–ö–æ–¥**:
```python
# –í VKClient —É–∂–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ
if source.last_checked:
    params_dict['start_time'] = int(source.last_checked.timestamp())
    # –°–æ–±–∏—Ä—ë—Ç —Ç–æ–ª—å–∫–æ –ø–æ—Å—Ç—ã –ü–û–°–õ–ï last_checked

# –í analyzer
if not content:
    logger.info("No new content to analyze")
    return None

# –ê–Ω–∞–ª–∏–∑ —Ç–æ–ª—å–∫–æ –Ω–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
analysis = await self.analyze_content(content, source)

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å –¥–∞—Ç–æ–π –°–ï–ì–û–î–ù–Ø (—Ç.–∫. —ç—Ç–æ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ)
analytics = await self._save_analysis(
    analysis_date=date.today(),  # OK –¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ
    # ...
)

# –û–±–Ω–æ–≤–∏—Ç—å last_checked
source.last_checked = datetime.now(UTC)
await source.save()
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç**:
- **1 –∑–∞–ø–∏—Å—å –≤ –¥–µ–Ω—å** (–µ—Å–ª–∏ –±—ã–ª –Ω–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç)
- Timeline —Ä–∞—Å—Ç—ë—Ç **–ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ**

---

## üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è

### **–î–ª—è Scenario #10 (User Activity)**

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å**: **–í–∞—Ä–∏–∞–Ω—Ç A (–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–Ω—è–º)**

**–ü–æ—á–µ–º—É**:
1. Event-based –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–±—É–µ—Ç –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ —Å–æ–±—ã—Ç–∏—è–º
2. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ö–æ—á–µ—Ç –≤–∏–¥–µ—Ç—å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å **–ø–æ –¥–Ω—è–º**
3. Timeline –¥–æ–ª–∂–µ–Ω –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å —ç–≤–æ–ª—é—Ü–∏—é –∑–∞ –≤–µ—Å—å –ø–µ—Ä–∏–æ–¥

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è**:
1. –î–æ–±–∞–≤–∏—Ç—å –º–µ—Ç–æ–¥ `analyze_content_by_days()` –≤ `AIAnalyzer`
2. –í `ContentCollector.collect_from_source()` –≤—ã–∑–≤–∞—Ç—å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫—É
3. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–Ω—è —Å–æ–∑–¥–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω—É—é –∑–∞–ø–∏—Å—å AIAnalytics

---

### **–î–ª—è –¥—Ä—É–≥–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤**

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å**: **–í–∞—Ä–∏–∞–Ω—Ç B (–∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π)**

**–ü–æ—á–µ–º—É**:
1. –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–Ω–µ event-based)
2. –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ 1 –∑–∞–ø–∏—Å—å –≤ –¥–µ–Ω—å
3. –≠–∫–æ–Ω–æ–º–∏—è LLM –∑–∞–ø—Ä–æ—Å–æ–≤

---

## üõ†Ô∏è –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–¥–∞

### **1. –î–æ–±–∞–≤–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä `analysis_date` –≤ `_save_analysis`**

**–§–∞–π–ª**: `app/services/ai/analyzer.py`

```python
async def _save_analysis(
    self,
    analysis_results: dict[str, Any],
    unified_summary: Optional[dict[str, Any]],
    source: Source,
    content_stats: dict[str, Any],
    platform_name: str,
    bot_scenario: Optional['BotScenario'] = None,
    topic_chain_id: Optional[str] = None,
    parent_analysis_id: Optional[int] = None,
    analysis_date: Optional[date] = None,  # ‚¨ÖÔ∏è NEW
) -> AIAnalytics:
    """Save comprehensive analysis results to database."""
    
    # Use provided date or default to today
    if analysis_date is None:
        analysis_date = date.today()
    
    # Check if analysis already exists for THIS date (not just today)
    existing_analysis = await AIAnalytics.objects.filter(
        source_id=source.id,
        analysis_date=analysis_date,  # ‚¨ÖÔ∏è CHANGED
        period_type=PeriodType.DAILY
    ).first()
    
    # ...
    
    # Create analytics record
    analytics = await AIAnalytics.objects.create(
        source_id=source.id,
        analysis_date=analysis_date,  # ‚¨ÖÔ∏è CHANGED
        # ...
    )
```

### **2. –î–æ–±–∞–≤–∏—Ç—å –º–µ—Ç–æ–¥ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –ø–æ –¥–Ω—è–º**

**–§–∞–π–ª**: `app/services/ai/analyzer.py`

```python
async def analyze_content_by_days(
    self,
    content: list[dict],
    source: Source,
    bot_scenario: Optional['BotScenario'] = None
) -> list[AIAnalytics]:
    """
    Group content by days and analyze each day separately.
    
    Args:
        content: List of content items
        source: Source being analyzed
        bot_scenario: Optional bot scenario
    
    Returns:
        List of AIAnalytics records (one per day)
    """
    from collections import defaultdict
    from datetime import datetime, timezone
    
    # Group content by day
    content_by_day = defaultdict(list)
    
    for item in content:
        # Extract publication date
        pub_date = item.get('published_at') or item.get('date') or item.get('created_at')
        
        # Convert to datetime
        if isinstance(pub_date, int):
            pub_date = datetime.fromtimestamp(pub_date, tz=timezone.utc)
        elif isinstance(pub_date, str):
            pub_date = datetime.fromisoformat(pub_date.replace('Z', '+00:00'))
        
        # Group by date
        day = pub_date.date() if pub_date else date.today()
        content_by_day[day].append(item)
    
    logger.info(f"Grouped {len(content)} items into {len(content_by_day)} days for source {source.id}")
    
    # Analyze each day
    analytics_list = []
    
    for day, day_content in sorted(content_by_day.items()):
        logger.info(f"Analyzing {len(day_content)} items for {source.id} on {day}")
        
        try:
            # Analyze content for this day
            analytics = await self.analyze_content(
                content=day_content,
                source=source,
                bot_scenario=bot_scenario,
                analysis_date=day  # ‚¨ÖÔ∏è PASS DATE TO analyze_content
            )
            
            analytics_list.append(analytics)
            
        except Exception as e:
            logger.error(f"Error analyzing day {day} for source {source.id}: {e}")
            continue
    
    logger.info(f"Created {len(analytics_list)} analytics records for source {source.id}")
    
    return analytics_list
```

### **3. –û–±–Ω–æ–≤–∏—Ç—å `analyze_content` –¥–ª—è –ø—Ä–∏—ë–º–∞ –¥–∞—Ç—ã**

```python
async def analyze_content(
    self,
    content: list[dict],
    source: Source,
    bot_scenario: Optional['BotScenario'] = None,
    topic_chain_id: Optional[str] = None,
    analysis_date: Optional[date] = None  # ‚¨ÖÔ∏è NEW
) -> AIAnalytics:
    # ...
    
    # Save analysis
    analysis = await self._save_analysis(
        analysis_results=analysis_results,
        unified_summary=unified_summary,
        source=source,
        content_stats=content_stats,
        platform_name=platform_name,
        bot_scenario=bot_scenario,
        topic_chain_id=topic_chain_id,
        analysis_date=analysis_date  # ‚¨ÖÔ∏è PASS TO _save_analysis
    )
    
    return analysis
```

### **4. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ collector**

**–§–∞–π–ª**: `app/services/monitoring/collector.py`

```python
async def collect_from_source(self, source, content_type="posts", analyze=True):
    # ... collect content ...
    
    if analyze and content:
        # Check if scenario requires day-by-day analysis
        is_event_based = False
        if source.bot_scenario and source.bot_scenario.scope:
            is_event_based = source.bot_scenario.scope.get('event_based', False)
        
        if is_event_based:
            # Analyze by days for event-based scenarios
            analytics_list = await self.ai_analyzer.analyze_content_by_days(
                content=content,
                source=source,
                bot_scenario=source.bot_scenario
            )
            logger.info(f"Created {len(analytics_list)} day-by-day analytics for source {source.id}")
        else:
            # Regular aggregated analysis
            analytics = await self.ai_analyzer.analyze_content(
                content=content,
                source=source,
                bot_scenario=source.bot_scenario
            )
            logger.info(f"Created aggregated analytics for source {source.id}")
    
    # ...
```

---

## üîß –ë—ã—Å—Ç—Ä—ã–π —Ñ–∏–∫—Å –¥–ª—è Source #19

### **1. –£–¥–∞–ª–∏—Ç—å —Ç–µ–∫—É—â—É—é –∑–∞–ø–∏—Å—å**

```sql
DELETE FROM social_manager.ai_analytics WHERE source_id = 19;
```

### **2. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å Scope —Å event_based**

```sql
UPDATE social_manager.bot_scenarios
SET scope = '{"event_based": true, "max_events_per_analysis": 50, "include_target_info": true}'::jsonb
WHERE id = 10;
```

### **3. –ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑ —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π**

```bash
# –ü–æ—Å–ª–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ analyze_content_by_days
python -m cli.scheduler run --once
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç**:
```sql
SELECT COUNT(*), MIN(analysis_date), MAX(analysis_date)
FROM social_manager.ai_analytics
WHERE source_id = 19;

-- –†–µ–∑—É–ª—å—Ç–∞—Ç:
count | min         | max
------+-------------+-------------
50+   | 2024-01-29  | 2025-10-17
```

---

## üìã –ò—Ç–æ–≥–æ–≤—ã–π —á–µ–∫-–ª–∏—Å—Ç

### **–ü—Ä–æ–±–ª–µ–º—ã**:
- [ ] ‚ùå –î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞ = `date.today()` ‚Üí –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç 00:00 (3:00 MSK)
- [ ] ‚ùå –¢–æ–ª—å–∫–æ 1 –∑–∞–ø–∏—Å—å –∑–∞ –≤–µ—Å—å –ø–µ—Ä–∏–æ–¥ ‚Üí –Ω–µ—Ç timeline

### **–†–µ—à–µ–Ω–∏—è**:
- [ ] ‚úÖ –î–æ–±–∞–≤–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä `analysis_date` –≤ `_save_analysis`
- [ ] ‚úÖ –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `analyze_content_by_days()`
- [ ] ‚úÖ –ü—Ä–æ–≤–µ—Ä—è—Ç—å `scope.event_based` –≤ collector
- [ ] ‚úÖ –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ø–æ –¥–Ω—è–º –¥–ª—è event-based —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤

### **–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ**:
- [ ] –£–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—É—é –∑–∞–ø–∏—Å—å source 19
- [ ] –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å `scope.event_based = true`
- [ ] –ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑
- [ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –º–Ω–æ–≥–æ)
- [ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å timeline –Ω–∞ dashboard

---

**–ê–≤—Ç–æ—Ä**: Factory Droid  
**–î–∞—Ç–∞**: 2025-10-18
